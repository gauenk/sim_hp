\documentclass[11pt]{article}
\input{preamble}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{hyperref,xcolor}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}

\newcommand{\algrule}[1][.2pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}
\author{Kent Gauen}
\title{Draft}

\begin{document}

\maketitle

\section{The Hawkes and Poisson Processes}

\subsection{The Hawkes Processes}

\noindent A Hawkes Process has a rate that depends on the history of the events. A common form of a Hawkes Process rate is the following:

\[
  \lambda^*(t)
  =
  \lambda(t|H)
  =
  \mu(t)
  +
  \sum_{(t',\kappa')\in H}\alpha(\kappa')\beta(t-t',\kappa')
\]

\noindent We call $\mu(t)$ the base rate and the $\alpha(\kappa')\beta(t-t',\kappa')$ term the offpsring intensity. These terms come from a different construction of the Hawkes process called the marked Poisson clustering process. Related work such as Shelton et al~\cite{AAAI1816985} uses the marked Poisson clustering process to do inference over the implicit tree structure of the Hawkes Process. This is useful in applications where the tree structure has a direct, useful interpretation; e.g. One can answer how crime in one geographic region will impact crime in other regions. For more on this one can reference~\cite{Rasmussen2013}. Running MCMC over the implicit tree structure of Hawkes process leads to complicated and localized MCMC updates, slowing the chain's mixing rate.

\subsection{The Poisson Processes}

\noindent A Poisson process samples discrete events in continuous time with a rate $\lambda(t)$. A Poisson process is called a homogenous Poisson process if $\lambda(t)$ is constant for all time $t$. Otherwise, a Poisson process is called a non-homogenous Poisson process. We note that conditioned on the history, a the Hawkes Process rate function is simply the rate function for a non-homogenous Poisson process.

\section{Algorithm}

We propose running MCMC via a Gibbs sampler directly over a set of virtual events $V$ and the Hawkes process trajectory $S$. We utilize the fact that for a fixed set of events $S$, the simulated Hawkes Process rate $\lambda^*(t)$ is a non-homogenous Poisson process. Thus, we can recover $S$ by thinning samples from a dominating Poisson process $\lambda(t)$, $\lambda(t) > \lambda^*(t)$ for all $t \in [0,T]$~\cite{pthin}. While the need for a dominating Poisson process rate implies clear theoretical restrictions to our algorithm, we demonstrate in Section~\ref{sec:tdb} this model can be applied to a variety of applications

The proposed generative model for observed events from a censored Hawkes Process is provided in Algorithm~\ref{alg:censored_hp}. We simulate virtual events $V\sim \text{PoissonProcess}(\lambda(t))$ and apply thinning to recover $S$ and $U$, $S \cup U = V$, with $S$ the Hawkes trajectory and $U$ the thinned events. This thinning can be interpreted as a binary mask $Z \in \{0,1\}^{|V|}$ applied to events $v_i \in V$ with $z_i = 1$ indicating $v_i \in S$ and $z_i = 0$ indicating $v_i \in U$. Next, we separate $S$ into two disjoint sets: the observed events $X$ and censored events $Y$. The censoring occurs with probability $\text{Bern}(\nu)$. %Noteably, our algorithm requires us to have a rate $\lambda(t)$ which dominates the Hawkes process rate for any time $\lambda^*(t)$. %This censoring can be interpreted as a binary mask $C \in \{0,1\}^{|S|}$ applied to events in $s_i \in S$ with $c_i = 1$ indicating the event is observed, $s_i \in X$, and $c_i = 0$ indicating the event is censored, $s_i \in Y$.

\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Hawkes process parameters $(\mu,\alpha,\beta,\gamma)$ and Censoring Parameter $\nu$}
  \KwOut{$X$, observed events sample from a censored Hawkes Process}
  \algrule
  $S = \emptyset$\;
  $V \sim \text{PoissonProcess}(\lambda(t))$\;
  \For{$v_i \in V$}{ % simulates the parents
    \If{w.p. $\frac{\lambda^*(v_i)}{\lambda(v_i)}$}{
      $\kappa_i \sim \gamma(\kappa_i | v_i)$\; % simulate the mark
      $S = S \cup (v_i,\kappa_i)$\;
    }
  }
  $z_i | s_i \sim \text{Bern}(\nu)$, $Z = \{z_i\}_{i=1}^{|V|}$\;
  $X = \{v_i : z_i = 1, z_i \in Z\}$, $Y = \{v_i : z_i = 0, z_i \in Z\}$\;
  \algrule
  \caption{Generative model for a censored Hawkes Process}
  \label{alg:censored_hp}
\end{algorithm}


Our Gibbs sampler targets the state $(S,V)$ or equivalently $(X,Y,U,Z)$. The Gibbs sampler alternates between simulating $U|X,Y \sim \text{PoissonProcess}(\lambda(t) - \lambda^*(t))$ and simulating censored events from virtual events $Z|X,Y,U$ with either Forward-Filtering Backward-Sampling (FFBS) or Metroplis-within-Gibbs algorithms. 


The FFBS algorithm allows our proposals to mode hop, giving our MCMC algorithm improved mixing speed. However, this comes with additonal computational complexity. The FFBS algorithm requires $O(N^2T)$ computation complexity for $N$ states. For a Hawkes Process, this set of states for $Z$ grows exponentially with $T$ to $N \approx 2^T$. This computational burdon can be mitigated by only allowing a window of memory, say $\tau$, the contribute to the current state space. Thus $N \leq O(2^\tau)$ for the entire simulation maintaining a computational complexity of $O(N^{2\tau} T)$. While this window does not make FFBS algorithm exact, a properly set $\tau$ provides analogous to computer precision error (to Rao: should we invest in a strong claim like this, or maybe somethig weaker? I guess this could be an alternative to exact but then our algorithm is approximate but idk if this matters.), say $10^{-9}$.


The naive alternative to FFBS is a Metropolis-within-Gibbs method of sequentially selecting censored events from $Z|X,Y,U$ and reconditioning the future samples from these selections, $z_{i+1} | X,Y,U,\{z_k\}_{k=1}^i$. However, this leads slow mixing and an additional set of acceptance probabilities. Additionall this naive method still requires $O(T)$ computation complexity. We experimentally demonstrate the advantage of FFBS in Section~\ref{sec:tbd}.


\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Observed Hawkes events $X$, censored Hawkes events $Y$, thinned events $U$, and the thinning mask $Z$.}
  \KwOut{Newly sampled censored Hawkes events $Y$, thinned events $U$, and thinning mask $Z$}
  \algrule
  Simulate $U|X,Y \sim \text{PoissonProcess}(\lambda(t) - \lambda^*(t))$\;
  Simulate $Z|X,Y,U$ using FFBS\;
  \algrule
  \caption{Gibbs Sampling Algorithm for Hawkes Process}
  \label{alg:gibbs_hp}
\end{algorithm}


\subsection{Resample thinned events conditions on Data}

To resample the thinned events of the Hawkes process conditioned on the data, we proposed using the FFBS algorithm. This improves our MCMC algorithms mixing rate with little additional computational complexity.



\newpage
\section*{Related Work}

Shelton et al~\cite{AAAI1816985}


\newpage
\section*{References}

\bibliography{ref}


\end{document}


% auto generate a title
% \AtBeginDocument{\maketitle}
