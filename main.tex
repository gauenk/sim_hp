\documentclass[11pt]{article}
\input{preamble}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{hyperref,xcolor}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}

\newcommand{\algrule}[1][.2pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}
\author{Kent Gauen}
\title{Draft}

\begin{document}

\maketitle

\section{The Hawkes and Poisson Processes}

\subsection{The Hawkes Processes}

\noindent A Hawkes Process has a rate that depends on the history of the events. A common form of a Hawkes Process rate is the following:

\[
  \lambda^*(t)
  =
  \lambda(t|H)
  =
  \mu(t)
  +
  \sum_{(t',\kappa')\in H}\alpha(\kappa')\beta(t-t',\kappa')
\]

\noindent We call $\mu(t)$ the base rate and the $\alpha(\kappa')\beta(t-t',\kappa')$ term the offpsring intensity. These terms come from a different construction of the Hawkes process called the marked Poisson clustering process. Related work such as Shelton et al~\cite{AAAI1816985} uses the marked Poisson clustering process to do inference over the implicit tree structure of the Hawkes Process. This is useful in applications where the tree structure has a direct, useful interpretation; e.g. One can answer how crime in one geographic region will impact crime in other regions. For more on this one can reference~\cite{Rasmussen2013}. Running MCMC over the implicit tree structure of Hawkes process leads to complicated and localized MCMC updates, slowing the chain's mixing rate.

\subsection{The Poisson Processes}

\noindent A Poisson process samples discrete events in continuous time with a rate $\lambda(t)$. A Poisson process is called a homogenous Poisson process if $\lambda(t)$ is constant for all time $t$. Otherwise, a Poisson process is called a non-homogenous Poisson process. We note that conditioned on the history, a the Hawkes Process rate function is simply the rate function for a non-homogenous Poisson process.

\section{Algorithm}

We propose running MCMC via a Gibbs sampler directly over a set of virtual events $V$ and the Hawkes process trajectory $S$. We utilize the fact that for a fixed set of events $S$, the simulated Hawkes Process rate $\lambda^*(t)$ is a non-homogenous Poisson process. Thus, we can recover $S$ by thinning samples from a dominating Poisson process $\lambda(t)$, $\lambda(t) > \lambda^*(t)$ for all $t \in [0,T]$~\cite{pthin}. While the need for a dominating Poisson process rate implies clear theoretical restrictions to our algorithm, we demonstrate in Section~\ref{sec:tdb} this model can be applied to a variety of applications

The proposed generative model for observed events from a censored Hawkes Process is provided in Algorithm~\ref{alg:censored_hp}. We simulate virtual events $V\sim \text{PoissonProcess}(\lambda(t))$ and apply thinning to recover $S$ and $U$, $S \cup U = V$, with $S$ the Hawkes trajectory and $U$ the thinned events. This thinning can be interpreted as a binary mask $Z \in \{0,1\}^{|V|}$ applied to events $v_i \in V$ with $z_i = 1$ indicating $v_i \in S$ and $z_i = 0$ indicating $v_i \in U$. Next, we separate $S$ into two disjoint sets: the observed events $X$ and censored events $Y$. The censoring occurs with probability $\text{Bern}(\nu)$. %Noteably, our algorithm requires us to have a rate $\lambda(t)$ which dominates the Hawkes process rate for any time $\lambda^*(t)$. %This censoring can be interpreted as a binary mask $C \in \{0,1\}^{|S|}$ applied to events in $s_i \in S$ with $c_i = 1$ indicating the event is observed, $s_i \in X$, and $c_i = 0$ indicating the event is censored, $s_i \in Y$.

\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Hawkes process parameters $(\mu,\alpha,\beta,\gamma)$ and Censoring Parameter $\nu$}
  \KwOut{$X$, observed events sample from a censored Hawkes Process}
  \algrule
  $S = \emptyset$\;
  $V \sim \text{PoissonProcess}(\lambda(t))$\;
  \For{$v_i \in V$}{ % simulates the parents
    \If{w.p. $\frac{\lambda^*(v_i)}{\lambda(v_i)}$}{
      $\kappa_i \sim \gamma(\kappa_i | v_i)$\; % simulate the mark
      $S = S \cup (v_i,\kappa_i)$\;
    }
  }
  $z_i | s_i \sim \text{Bern}(\nu)$, $Z = \{z_i\}_{i=1}^{|V|}$\;
  $X = \{v_i : z_i = 1, z_i \in Z\}$, $Y = \{v_i : z_i = 0, z_i \in Z\}$\;
  \algrule
  \caption{Generative model for a censored Hawkes Process}
  \label{alg:censored_hp}
\end{algorithm}


Our Gibbs sampler targets the state $(S,V)$ or equivalently $(X,Y,U,Z)$. The Gibbs sampler alternates between simulating $U|X,Y \sim \text{PoissonProcess}(\lambda(t) - \lambda^*(t))$ and simulating censored events from virtual events $Z|X,Y,U$ with either Forward-Filtering Backward-Sampling (FFBS) or Metroplis-within-Gibbs algorithms. 


The FFBS algorithm allows our proposals to mode hop, giving our MCMC algorithm improved mixing speed. However, this comes with additonal computational complexity. The FFBS algorithm requires $O(N^2T)$ computation complexity for $N$ states. For a Hawkes Process, this set of states for $Z$ grows exponentially with $T$ to $N \approx 2^T$. This computational burdon can be mitigated by only allowing a window of memory, say $\tau$, the contribute to the current state space. Thus $N \leq O(2^\tau)$ for the entire simulation maintaining a computational complexity of $O(N^{2\tau} T)$. While this window does not make FFBS algorithm exact, a properly set $\tau$ provides analogous to computer precision error (to Rao: should we invest in a strong claim like this, or maybe somethig weaker? I guess this could be an alternative to exact but then our algorithm is approximate but idk if this matters.), say $10^{-9}$.


The naive alternative to FFBS is a Metropolis-within-Gibbs method of sequentially selecting censored events from $Z|X,Y,U$ and reconditioning the future samples from these selections, $z_{i+1} | X,Y,U,\{z_k\}_{k=1}^i$. However, this leads slow mixing and an additional set of acceptance probabilities. Additionall this naive method still requires $O(T)$ computation complexity. We experimentally demonstrate the advantage of FFBS in Section~\ref{sec:tbd}.


\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Observed Hawkes events $X$, censored Hawkes events $Y$, thinned events $U$, and the thinning mask $Z$.}
  \KwOut{Newly sampled censored Hawkes events $Y$, thinned events $U$, and thinning mask $Z$}
  \algrule
  Simulate $U|X,Y \sim \text{PoissonProcess}(\lambda(t) - \lambda^*(t))$\;
  Simulate $Z|X,Y,U$ using FFBS\;
  \algrule
  \caption{Gibbs Sampling Algorithm for Hawkes Process}
  \label{alg:gibbs_hp}
\end{algorithm}

\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Observed Hawkes events $X$, censored Hawkes events $Y$, thinned events $U$}
  \KwOut{Newly sampled censored Hawkes events $\widetilde{Y}$ and thinned events $\widetilde{U}$}
  \algrule
  $L = Y \cup U$\;
  \For{$l_i \in L$}{
    $Y = Y \setminus \{l_i\}$, $U = U \setminus \{l_i\}$\;
    \uIf{w.p. $p(l_i|X,Y,U)/p(l_i|\lambda)$}{
      $Y = Y \cup \{l_i\}$\;
    }\uElse{
      $U = U \cup \{l_i\}$\;
    }
  }
  \algrule
  \caption{Thinning procedure for resampling censored Hawkes events}
  \label{alg:gen_hp_thin}
\end{algorithm}


\subsection{Resample thinned events conditions on Data}

To resample the thinned events of the Hawkes process conditioned on the data, we proposed using the FFBS algorithm. This improves our MCMC algorithms mixing rate with little additional computational complexity.

\begin{comment}

\vspace{1cm}
\noindent\textbf{Probability Event is Thinned}


\[
  P(l_i | X, Y ,U) 
  =
  P(l_i | V)/ P(l_i | \lambda)
  =
  \text{PoissonProcess}(l_i)
\]

\vspace{1cm}
\noindent\textbf{Probability Event is Real}

\noindent Let's compute the probability the event is real. We include a base event at $t = 0$ so all observed events are children and immigrants are direct descendants of this base event. We will suppress the dependence on the parameters $(\vtheta,\alpha,\lambda)$ in our writing. Let $V' = V \setminus \{v_i\}$, $Z' = Z \setminus \{z_i\}$,

\[
  P(a_i \text{ is real } | A \setminus \{a_i\}, X, \vtheta, \alpha, \lambda)
\]
\[
  =
  P(a_i \text{ is real} | A \setminus \{a_i\}, X)
  =
  P(a_i \text{ is real} | V', Z', X)
  \propto
  P(a_i \text{ is real}, V', Z', X)
\]

\[
  =
  P(a_i \text{ is a child of any past event}, V', Z', X)   
\]

\[
  =
  P((v_i,z_i=1) , V', Z', X)   
\]

\noindent In words, we want the probability the virtual event was censored, indicated by $z_i = 1$. Note $v \in V$ is the event time of an event sampled from the Poisson Process and $z \in Z$ is a binary number indicating if the event is virtual or real (censored). Let's define $V_i,Z_i$ to be all the events before $a_i$ and including $(v_i,1)$. E.g. $V_i = \{a_k\}_ {k=1}^{i-1} \cup \{a_i\}$ and $Z_i = \{z_k\}_{k=1}^{i-1} \cup \{1\}$. Now we break up the distribution into its components:

\[
  =
  P(V')P(Z')P(a_i, X | V', Z')
  =
  P(V')P(Z')P(a_i, X | V_i, Z_i)
\]

\[
  =
  \text{PoissonProc}(V'; \lambda)
  \;
  \prod_{z \in Z'}\text{Bernoulli}(z; \alpha) 
  \;
  P(a_i, X | V_i, Z_i)  
\]

\[
  P(a_i, X | V_i, Z_i)
  =
  \sum_{p(a_i) \in \mathcal{P}}
  P(\text{not thinning } a_i \text{ with parent } p(a_i))
  \text{HawkesProc}(a_i,p(a_i),X,Y; \vtheta)
\]

\noindent Recall $a_i = (v_i,z_i)$ and if $a_i$ is a real, censored event then $a_i = (v_i, 1)$. The $\mathcal{P}$ is the set of possible parents for $a_i$ and $p(a_i)$ is the parent event. The $\text{HawkesProc}(\cdots)$ is the probability of the trajectory an event at time $v_i$ inserted with parent $p(a_i)$.


This is easy to compute when $\mathcal{P}$ is small, but this could become expensive if the algorithm is run over an interval with many events. We can make this efficient by assigning a memory threshold; e.g. if the contribution to the current probability is less than say $10^{-6}$ then we ignore the remaining terms. This will something I will explore more during the project.

\vspace{1cm}
\noindent\textbf{Probability Event is Virtual}

\noindent Next we write down the probability $a_i$ is a virtual event. This is very similar to our previous derivation, but now we change the final probability:

\[
  P(a_i \text{ is virtual } | A \setminus \{a_i\}, X, \vtheta, \alpha, \lambda)
\]
\[
  =
  P(a_i \text{ is virtual } | A \setminus \{a_i\}, X)
  =
  P(a_i \text{ is virtual } | V', Z', X)
  \propto
  P(a_i \text{ is virtual }, V', Z', X)
\]

\[
  =
  P(V')P(Z')P(a_i, X | V', Z')
\]

\[
  =
  \text{PoissonProc}(V'; \lambda)
  \;
  \prod_{z \in Z'}\text{Bernoulli}(z; \alpha) 
  \;
  P(a_i, X | V_i, Z_i)  
\]

\[
  P(a_i, X | V_i, Z_i)
  =
  \text{HawkesProc}(X,Y; \vtheta)
  \sum_{p(a_i) \in \mathcal{P}}
  P(\text{thinned } a_i \text{ with parent } p(a_i))
\]


\noindent Again, this summation depends on the number of the events over the interval. We will explore more options to make this efficient in the future.



\newpage
\section*{Related Work}

Shelton et al~\cite{AAAI1816985}


\newpage
\section*{References}

\bibliography{ref}


\end{document}


% auto generate a title
% \AtBeginDocument{\maketitle}
