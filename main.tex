\documentclass[11pt]{article}
\input{preamble}
\usepackage{hyperref,xcolor}
\usepackage{comment}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{parskip}

\newcommand{\algrule}[1][.2pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}
\author{Kent Gauen}
\title{Draft}

\begin{document}

\maketitle

\section{The Hawkes and Poisson Processes}

\subsection{The Hawkes Processes}

\noindent A Hawkes Process is a point process with a rate function that depends on the history of the events. A common form of a Hawkes Process rate is the following:

\begin{align}
  \lambda^*(t)
  =
  \lambda(t|H)
  =
  \mu(t)
  +
  \sum_{(t',\kappa')\in H}\alpha(\kappa')\beta(t-t',\kappa')
\end{align}

\noindent We call $\mu(t)$ the base rate and the $\alpha(\kappa')\beta(t-t',\kappa')$ term the offspring intensity. These terms come from the marked Poisson clustering process construction of the Hawkes process. In short, this construction builds a Hawkes process by sampling events directly from an latent family-tree. Related work such as Shelton et al~\cite{AAAI1816985} uses the marked Poisson clustering process for inference directly over the implicit tree structure of the Hawkes Process. In some applications the tree structure has a useful interpretation; e.g. One can answer how crime in one geographic region will impact crime in other regions. For more on this one can read~\cite{Rasmussen2013}. Running MCMC over the implicit tree structure of a Hawkes process leads to complicated and localized MCMC updates, resulting in slow mixing rate of the chain.

\subsection{The Poisson Processes}

\noindent A Poisson process samples discrete events in continuous time with a rate $\eta(t)$. A Poisson process is called a homogeneous Poisson process if $\eta(t) = C$ is constant for all time $t$. Otherwise, a Poisson process is called a non-homogeneous Poisson process. We note that conditioned on the events, a Hawkes Process rate function is simply the rate function of a non-homogeneous Poisson process. That is we have $\lambda^*(t) = \lambda(t | S)$ is only a function of time.

\section{Algorithms}

We propose running MCMC via a Gibbs sampler over the generative process outlined in Algorithm~\ref{alg:censored_hp}. We utilize the fact that for a fixed set of events $S$, the simulated Hawkes Process rate $\lambda^*(t)$ yields a non-homogeneous Poisson process. Thus, we can recover event times in $S$ via thinning from a dominating Poisson process with rate function $\eta(t)$, $\eta(t) > \lambda^*(t)$ for all $t \in [0,T]$~\cite{pthin}. While the need for a dominating rate function implies clear theoretical restrictions to our algorithm, we demonstrate in Section~\ref{sec:tdb} the clear practical utility despite this restriction.

\subsection{Generative Process}


The proposed generative model for observed events from a censored Hawkes Process is provided in Algorithm~\ref{alg:censored_hp}. We simulate virtual events $V\sim \text{PoissonProcess}(\lambda(t))$. Next, we apply the Poisson Thinning Theorem to recover $S$ and $U$, $S \cup U = V$, with $S$ the Hawkes trajectory and $U$ the thinned events. This thinning can be interpreted as a binary mask $L \in \{0,1\}^{|V|}$ applied to events $v_i \in V$ with $l_i = 1$ indicating $v_i \in S$ and $l_i = 0$ indicating $v_i \in U$. Finally to model the censored events, we separate $S$ into two disjoint sets: the observed events $X$ and censored events $Y$, $S = X \cup Y$. The censoring occurs with probability $\text{Bern}(\nu)$. Once again, we can see step as first simulating a binary mask $Z \in \{0,1\}^{|S|}$ and then splitting $S$ into the disjoint sets $X$ and $Y$ accordingly. The probabiliy of our generative model is given with  $S_k = \{v_j \in V |\; v_j < v_k, l_j = 1\}$ as

\begin{align}
P(V,L,Z)
=
\text{PoissonProc}(V\;;\eta(t))\;
\prod_{i=1}^{|L|} \text{Bern}\left(l_i\;;\frac{\lambda(v_i | S_k)}{\eta(v_i)}\right)
\prod_{i=1}^{|Z|} \text{Bern}(z_i\;;\nu)
\end{align}

\noindent or written explicitly with data as

\begin{align}
  P(U,Y,X)
  =
  \text{PoissonProc}(X \cup Y\;;\lambda(t|S_k))\;
  \text{PoissonProc}(U\;;\eta(t) - \lambda(t|S_k))\; \nonumber \\
  \prod_{i=1}^{|U| + |S|} \text{Bern}\left(\mathbf{1}_{w_i \in S}\;;\frac{\lambda(v_i | S_k)}{\eta(v_i)}\right)  
  \prod_{i=1}^{|S|} \text{Bern}(\mathbf{1}_{s_i \in X}\;;\nu)
\end{align}

When writing algorithm details, it is convenient to write the binary masks $L$ and $Z$ as one variable $G$. We consider each event $v \in V$ to be one of the following types. Type $1$ events are observered Hawkes events and correspond to $(l,z,g) = (1,1,1)$. Type $0$ events are missing Hawkes events and correspond to $(l,z,g) = (1,0,0)$. Type $-1$ events are thinned events from the dominating Poisson Process and correspond to $(l,\cdot,g) = (0,\cdot,-1)$. Notice there are no $z$ values for Type $0$ events which is precisely why the variable $G$ is convenient. Now we can re-write the probability of our generative process with $S_k = \{v_j \in V |\; v_j < v_k, g_j \in \{0,1\}\}$ as

\begin{align}
P(V,G)
=
\text{PoissonProc}(V\;;\eta(t))\;
  \prod_{i=1}^{|G|} \text{Bern}\left(\mathbf{1}[g_i\in\{0,1\}]\;;\frac{\lambda(v_i | S_k)}{\eta(v_i)}\right)
  \text{Bern}(g_i\;;\nu)\mathbf{1}[g_i \in \{0,1\}]
\end{align}



%Noteably, our algorithm requires us to have a rate $\lambda(t)$ which dominates the Hawkes process rate for any time $\lambda^*(t)$. %This censoring can be interpreted as a binary mask $C \in \{0,1\}^{|S|}$ applied to events in $s_i \in S$ with $c_i = 1$ indicating the event is observed, $s_i \in X$, and $c_i = 0$ indicating the event is censored, $s_i \in Y$.

\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Hawkes process rate function $\lambda(t|\mathcal{H})$, a dominating Poisson process rate $\eta(t)$, and the censoring parameter $\nu$}
  \KwOut{$X$, observed events sample from a censored Hawkes Process}
  \algrule
  $S = \emptyset$, the Hawkes events\;
  $V \sim \text{PoissonProcess}(\eta(t))$\;
  $l_i | \{v_{k}\}_{k=1}^i \sim \text{Bern}(\frac{\lambda(v_i | S_i)}{\eta(v_i)})$ with
  $S_i = \{v_j \in V|\; v_j < l_i, l_j = 1\}$\;
  $L = \{l_i\}_{i=1}^{|V|}$, the thinning mask\;
  $S = \{v_i : l_i = 1, l_i \in L, v_i \in V\}$\;
  $U = \{v_i : l_i = 0, l_i \in L, v_i \in V\}$\;
  %\For{$v_i \in V$}{ % simulates the parents
  %  \uIf{w.p. $\frac{\lambda(v_i | S)}{\eta(v_i)}$}{
   %   $m_i \sim p_m(m_i | v_i)$\; % simulate the mark
    %  $S = S \cup (v_i,\kappa_i)$\;
    %}\Else{
    %    $U = U \cup \{v_i\}$
    %}
  %}
  $z_i | s_i \sim \text{Bern}(\nu)$\;
  $Z = \{z_i\}_{i=1}^{|S|}$, the censoring mask\;
  $X = \{s_i : z_i = 1, z_i \in Z\}$\;
  $Y = \{s_i : z_i = 0, z_i \in Z\}$\;
  \algrule
  \caption{Generative model for a censored Hawkes Process}
  \label{alg:censored_hp}
\end{algorithm}


\subsection{Gibbs Sampler}

Our Gibbs sampler targets the state $G,U,Y | X, \mTheta$. The Gibbs sampler alternates between (i) simulating thinned events $U|Y_t,X_t,\mTheta \sim \text{PoissonProcess}(\eta(t) - \lambda(t | S_t))$ where $Y_t,X_t$ are all events before time $t$, and (ii) swapping thinned and missing events $G|U,Y,X,\mTheta$ with either a Metroplis-within-Gibbs algorithm or Forward-Filtering Backward-Sampling (FFBS). This procedure is outlined in Algorithm~\ref{alg:gibbs_hp}.



\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Observed Hawkes events $X$, censored Hawkes events $Y$, thinned events $U$, and event types $G$.}
  \KwOut{Newly sampled censored Hawkes events $Y$, thinned events $U$, and event types $G$}
  \algrule
  Simulate $U|X,Y,\mTheta \sim \text{PoissonProcess}(\eta(t) - \lambda(t | X_t, Y_t))$ with $X_t,Y_t$ the restricted set of events before time $t$\;
  Simulate $G|U,Y,X,\mTheta$ via Metropolis-within-Gibbs or FFBS\;
  \algrule
  \caption{Gibbs Sampling steps for Hawkes Process}
  \label{alg:gibbs_hp}
\end{algorithm}


\subsection{Metropolis-within-Gibbs}


\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{All event times $V = X \cup Y \cup U$, event types $\widetilde{G}$}
  \KwOut{New event types $G$}
  \algrule
  $G := {\tt copy}(\widetilde{G})$ ( I feel like this is silly. I really don't want output since this operation is inplace)\;
  \For{$g_i \in G$}{
    \uIf{$g_i = 0$ }{
      with probability $\alpha^i_{y2u}$, move event from $Y$ to $U$\;
      or equivalently $g_i = 0 \to g_i = -1$\;
    }\uElseIf{$g_i = -1$}{
      with probability $\alpha^i_{u2y}$, move event from $U$ to $Y$\;
      or equivalently $g_i = -1 \to g_i = 0$\;
    }
  }
  \algrule
  \caption{Metropolis Within Gibbs}
  \label{alg:gibbs_hp}
\end{algorithm}

We write $G^{i}_{y2u} := (G\setminus g_i)\cup\{g_i = -1\}$ and $G^{i}_{u2y} := (G\setminus g_i)\cup\{g_i = 0\}$. Then we have

\begin{align}
  \alpha_{y2u}^i
  =
  \frac{P(V,G^{i}_{y2u})}{P(V,G)}
  &&
  \alpha_{y2u}^i
  =
  \frac{P(V,G^{i}_{u2y})}{P(V,G)}
\end{align}


\subsection{FFBS}

For FFBS we need a Markov transition matrix. To accomplish this, we introduce the variable $R$ with $|R| = |V|$. The value $r_i = k$ corresponds to the index of the event furthest in the history to contribute meaningfully to the Hawkes process rate function. Aprior we define this length of time to be $\tau$. Thus we construct $R$ as the following: $r_1 = 0$ and 

\begin{align}
  r_i = \argmax_{k}\{ v_i - v_k < \tau : k < i\}
\end{align}

With $R$ we can write the probability as the following equation

\begin{align}
  P(V,G)
  =
  p(v_1,g_1)
  \prod_{i=2}^{|V|}P(v_{i}|v_{1:i-1})
  P(g_{i}|g_{1:i-1},v_{1:i-1})\\
  \approx
  P(V,G|R)
  =
  \prod_{i=1}^{|V|-1}P(v_{i}|v_{r_{i}:i-1})
  P(g_{i}|g_{r_{i}:i-1},v_{r_{i}:i-1})
\end{align}

% should we call it P(V,G | R) though?

Note this is not quite a semi-Markov process, since we do not assume we must return to the base rate $\lambda^*(t) = \mu(t)$. Rather, we still allow self-exciting events to modulate the rate function over time. The recursion for the forward algorithm can be written as:

\begin{align}
  P(g_i,v_{1:i})
  =
  P(v_i | v_{1:i-1}) \underbrace{\sum_{i-1}\cdots\sum_{r_i}}_{k_i\;:=\;i-r_i-1}p(g_i|g_{r_i:i-1},v_{1:i-1}) p(g_{i-1},v_{1:i-1})
\end{align}


Each step of this recursion requires computation of $O(2^{k_i})$ compared to the usual computation of $O(2)$ for a Markov transition. If this additional computation is too cumbersome, say $k_i  >> 20$, then further approximations can be used within the FFBS algorithm such as Particle MCMC. 


\subsection{Sampling Ancestry}

Once the MCMC chain has mixed and we have aggregated samples from the posterior, we can use the samples $N$ samples $\{S^i\}_{i-1}^N$ to estimate the embedded Poisson cluster process. We let $A$ be the functional, $A:[0,T]\times[0,T] \to [0,1]$, and say $A(X_i,X_j) $ is the probability $X_j$ is a descendants of $X_i$. To target the distribution $P(A | X)$, we use Gibbs sampling to (i) simulate ancestry from the full Hawkes Process $P(A | X, Y)$ and (ii) resample (via Algorithm~\ref{alg:gibbs_hp}) the missing Hawkes events $P(Y | X)$. Thus we can estimate the ancestry via Gibbs sampling, as outlined in Algorithm~\ref{alg:ancestry}

\begin{algorithm}[h]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwIn{Observed events $X$}
  \KwOut{Samples of $A$}
  \algrule
  Sample $P(A | X, Y)$\;
  Sample $P(Y | X)$ via Gibbs sampling (c.f. Algorithm~\ref{alg:gibbs_hp})\;
  \algrule
  \caption{Sampling Ancestry via Gibbs Sampling}
  \label{alg:ancestry}
\end{algorithm}




% (ii) set ancenstry | X, Y? is it easy to sample? hawkes kernel with bounded support. truncated hawkes process (in practice) then its linear.

% (iii) SA-AA for algorithm 3

% bounded impact and lambda(t) extensions of core idea.

\newpage
\section*{Related Work}

Shelton et al~\cite{AAAI1816985}


\newpage
\section*{References}

\bibliography{ref}


\end{document}


% auto generate a title
% \AtBeginDocument{\maketitle}
